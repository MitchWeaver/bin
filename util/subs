#!/bin/sh -e
#
# http://github.com/mitchweaver/bin
#
# play youtube subscriptions with dmenu, mpv, and youtube-dl
#
# deps: sed, sort, paste, head, seq, awk
#       a video player, a menu program, youtube-dl 
#

# -*-*-*-*-*-*- Settings -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-
opml_file=${HOME}/files/subs.opml
menuprog="menu -p Subscriptions:"
videoprog="mpv --really-quiet --sub-auto=no --title=mpv --input-ipc-server=/tmp/mpvsocket $@"
tmpdir=/tmp/yt
COUNT=500
# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*

case "$1" in -m|-nv) set -- "--no-video" ; esac

export LC_ALL=C
mkdir -p "$tmpdir"
trap 'rm -r "$tmpdir" 2>/dev/null' EXIT INT KILL TERM

# parse the terrible xml that is our opml subscription file
# dump the channel names and their rss links to corresponding files
cat "$opml_file" | tr '><' '\n' | \
    sed -E -e 's/" title.*.xmlUrl="/\n/g' \
           -e 's/(outline text="|" \/)//g' \
           -e '/^$/d' \
           -e '1,7d' | \
    while IFS='' read -r line ; do
        case $line in
            *http*) echo "$line" >>"$tmpdir"/rss_links ;;
                 *) echo "$line" >>"$tmpdir"/chan_names
        esac
    done

# takes a channel rss feed link and creates files of titles, urls, dates, and channels
get_vids() {
    link="$1"
    num=$2
    echo "* fetching $link"
    curl -q -s -L --url $link >"$tmpdir"/dl$num || return 1

    date="$(grep '<published>' "$tmpdir"/dl$num | \
        sed -E -e 's/( )*<\/?published>//g' -e 's/\+.*//g' -e '1d')"

    echo "$date" >>"$tmpdir"/vid_dates$num

    grep '<media:title>' "$tmpdir"/dl$num | \
        sed -E 's/( )*<\/?media:title>//g' >>"$tmpdir"/vid_titles$num

    grep '<media:content url=' "$tmpdir"/dl$num | \
        sed -E -e 's/( )*<\/?media:content url="//g' \
               -e 's/" type.*//g' >>"$tmpdir"/vid_urls$num

    for i in $(seq 1 $(echo "$date" | wc -l)) ; do
        sed -n "${num}p" "$tmpdir"/chan_names >>"$tmpdir"/vid_chans$num
    done

    rm "$tmpdir"/dl$num
}

echo "1. fetching ..."
count=0
while IFS='' read -r link ; do
    count=$(( $count + 1 ))
    get_vids "$link" $count &
done < "$tmpdir"/rss_links
wait

# concantenate all subfiles into collated files
for i in $(seq 1 $(cat "$tmpdir"/rss_links | wc -l)) ; do
    for file in vid_dates vid_chans vid_titles vid_urls ; do
        if [ -f "$tmpdir"/"$file"$i ] ; then
            cat "$tmpdir"/"$file"$i >>"$tmpdir"/$file
            rm "$tmpdir"/"$file"$i
        fi
    done
done

# concantenates files side by side, line by line
# note: paste can't use multi-char delimiters, and it cant do full unicode
#       using '`' as a junk character here so it can later be split into
#       my desired delimiter with a sed replace
echo "2. concatenating ..."
paste -d '|' "$tmpdir"/vid_dates \
             "$tmpdir"/vid_chans \
             "$tmpdir"/vid_titles \
             "$tmpdir"/vid_urls >"$tmpdir"/concat

# clean up any irregularities in the final file
sed -E -i 's/( )?<\/?media:description>.//' "$tmpdir"/concat

# now that we have all our information in the file, since we gathered them
# channel by channel we want to collate them based on date
echo "3. sorting ..."
sort -r "$tmpdir"/concat -o "$tmpdir"/concat

echo 'done!'

# send through our discovered info, nicely formatted, to dmenu
choice=$(awk -F '|' '{print "["$2"]" " - " $3}' "$tmpdir"/concat | \
    head -n $COUNT | $menuprog | sed -e 's/^.*- //')

[ "$choice" ] || exit 1

# read in the final concatenated file, and compare against our dmenu choice
while IFS='' read -r line ; do
    if echo $line | grep "$choice" >/dev/null ; then
        set -- $(echo $line | tr '|' ' ')
        while [ $# -ne 1 ] ; do shift ; done
        url=$(echo $1 | sed 's|v/|watch?v=|')
    fi
done <"$tmpdir"/concat

# make sure we didn't back out of dmenu before trying to run
if [ "$url" ] ; then
    printf "\n%s\n%s\n" "$choice" "playing $url"
    $videoprog "$url" &
    wait
fi
