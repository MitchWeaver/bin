#!/bin/sh
#
# http://github.com/mitchweaver/bin
#
# Watch your youtube subscriptions via curl, dmenu, and mpv
#
# The $SUBS_FILE is a text file containing usernames or channel IDs,
# comments and blank lines are ignored.
#
# Example:
#  ______________________
# |                      |
# |# Glink               |
# |GlinkLegend           |
# |                      |
# |# Vsauce              |
# |Vsauce                |
# |                      |
# |# Abroad in Japan     |
# |cmbroad44             |
# |                      |
# |______________________|
#

# -*-*-*-*-* Settings *-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*--*-*-*
: ${SUBS_FILE:=~/files/subs.txt}
: ${MENU_PROG:=dmenu}
: ${PLUMBER:=opn}
SUBS=${XDG_CACHE_HOME:-~/.cache}/subs
SUBS_LINKS=$SUBS/subs_links
SUBS_CACHE=$SUBS/subs_info
# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*

msg() { printf '[*] %s\n' "$*" ; }
die() { >&2 msg "$*" ; exit 1  ; }

usage() {
    >&2 printf '%s\n' 'Usage: subs [-g gen links] [-u update cache]'
    exit 1
}

# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
# Synopsis: $SUBS_FILE [txt] -> $SUBS_LINKS [xml links]
#
# Updates local cache of xml subscription links from the
# subscription file containing either usernames or channel ids.
# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
gen_links() {
    :>"$SUBS_LINKS"

    count=0
    total=$(sed -e '/^$/d' -e '/^#/d' <"$SUBS_FILE" | wc -l)

    while read -r line ; do
        # ignore comments and blank lines
        case $line in ''|' '|'#'*) continue ;; esac

        count=$(( $count + 1 ))
        printf "[%s/%s] fetching %s\n" $count $total "$line"

        case $line in
            UC*)
                # YT channel IDs always begin with 'UC' and are 24 chars long
                if [ ${#line} -eq 24 ] ; then
                    printf 'https://youtube.com/feeds/videos.xml?%s\n' \
                        "channel_id=$line" | tee -a "$SUBS_LINKS"
                    continue
                fi
        esac

        # otherwise we are given a username, we must find out its channel ID
        curl -sfL --retry 3 "https://youtube.com/user/$line/about" | \
            grep channel_id | \
            sed -e 's|www\.||' -e 's|.*href="||' -e 's|">.*||' | \
            tee -a "$SUBS_LINKS"

    done <"$SUBS_FILE"
}

# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
# Synopsis: $1 [LINK] -> $SUBS_CACHE/$chan_name/concat [CHANNEL INFO]
#
# Takes a channel rss feed link and creates a file
# with a line of its videos dates, titles, and urls.
# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
get_vids() {
    data=$(curl -sL "$1")

    chan_name=${data%%</name*}
    chan_name=${chan_name##*name>}
    mkdir -p "$SUBS_CACHE/$chan_name"

    printf "%s\n" "$data" | \
    while read -r line ; do
        case $line in
            *'<published>'*)
                line=${line%+00:*}
                line=${line#*<published>}
                printf '%s\n' "$line" >>"$SUBS_CACHE/$chan_name/dates"
                ;;
            *'<media:title>'*)
                line=${line%</*}
                line=${line#*:title>}
                printf '%s\n' "$line" >>"$SUBS_CACHE/$chan_name/titles"
                ;;
            *'www.youtube.com/v/'*)
                line=${line#*\"}
                line=${line%\?ver*}
                printf '%s\n' "$line" >>"$SUBS_CACHE/$chan_name/urls"
                ;;
        esac
    done

    while read -r line ; do
        printf '%s\n' "$chan_name"
    done <"$SUBS_CACHE/$chan_name/dates" >"$SUBS_CACHE/$chan_name/chan"

    # paste together a line of the separate data points, separated
    # by a delimiter '`' that we can break it apart with later
    paste -d '`' "$SUBS_CACHE/$chan_name/dates" \
                 "$SUBS_CACHE/$chan_name/chan" \
                 "$SUBS_CACHE/$chan_name/titles" \
                 "$SUBS_CACHE/$chan_name/urls" \
                 >"$SUBS_CACHE/$chan_name"/concat
}

# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
# Updates the local cache of subscriptions. ([-u] flag)
# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
update_subs() {
    [ -f "$SUBS_LINKS" ] || die Subs links have not been generated.

    rm -r "${SUBS_CACHE:-?}" 2>/dev/null ||:

    while read -r link ; do
        msg fetching vids from "${link#*nel_}"
        get_vids "$link"
    done <"$SUBS_LINKS"
}

# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
# Sort through the current cache of subscriptions, sort by
# date uploaded, and present to ${MENU_PROG} for selection.
# Finally, pipe this result to the ${PLUMBER}.
# -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
view_subs() {
    [ -d "$SUBS_CACHE" ] || die Subs cache has not been retrieved.
    tmp=/tmp/subs-$$

    printf "%sconcat \n" "$SUBS_CACHE"/*/ | \
    while read -r line ; do
        cat "$line"
    done | sort -r >$tmp

    sel=$(\
        while read -r line ; do
            chan=${line#*\`}
            chan=${chan%%\`*}
            title=${line#*$chan\`}
            title=${title%%\`*}
            printf '[%s] %s\n' "$chan" "$title"
        done <$tmp | ${MENU_PROG:-dmenu} -p 'Subs:'\
    )
    # if you want to also display the date use this:
    # date=${line%%\`*}

    if [ "$sel" ] ; then
        chan=${sel#\[}
        chan=${chan%%\]*}
        title=${sel#*\] }
        url=$(grep "$title" "$SUBS_CACHE/$chan/concat")
        url=${url##*\`}
        printf '%s\n' "$url" | sed 's|/v/|/watch?v=|' | \
            tee /dev/tty | ${PLUMBER:-opn}
    fi

    rm $tmp
}

main() {
    mkdir -p "$SUBS"

    if [ "$1" ] ; then
        while [ "$1" ] ; do
            case $1 in
                -g) gen_links ;;
                -u) update_subs ;;
                 *) usage
            esac
            shift
        done
    else
        view_subs
    fi
}

main "$@"
